
# 0. Attention 的本质
用一句话理解注意力机制：

> **我想要获取信息（Query），先去查询一组候选信息（Key），根据相关性打分，再按分数对信息（Value）加权求和，得到最终结果。**

- **Q（Query）**：当前位置/当前 token 想要“查询什么”
- **K（Key）**：每个位置提供的“索引”
- **V（Value）**：每个位置真正的“内容信息”
- **score(Q,K)**：计算相关性（不同注意力的核心区别）
- **softmax**：将分数归一化为注意力权重
- **加权求和**：按权重聚合信息，得到输出

---

# 1. 按「打分方式」分类：如何计算 Q 与 K 的相关性

## 1.1 Additive Attention（Bahdanau，加性注意力）
**直观理解**：
不直接判断 Q 和 K 的相似度，而是放进一个小网络里，让模型自己学习“像不像”。

**核心做法**：
将 Q 和 K 拼接，通过一层或多层 MLP 输出相关性分数。

**优点**：
- 表达能力强，可学习复杂匹配规则
- 低维、小数据场景下更稳定

**缺点**：
- 计算量大，速度慢
- 不如点积适合 GPU 并行

**常用场景**：
- 传统 Seq2Seq（RNN/LSTM）的编码器-解码器注意力
- 现代 Transformer 中较少单独使用

---

## 1.2 Dot-Product Attention（Luong，点积注意力）
**直观理解**：
直接计算 Q 和 K 的内积，结果越大表示越相关。

**核心做法**：
$$
\text{score} = q^\top k
$$

**优点**：
- 计算简单、速度快
- 天然适合矩阵乘法与 GPU 加速

**缺点**：
- 维度增大时，分数易爆炸，softmax 分布过于尖锐，训练不稳定

**常用场景**：
- Transformer 之前的注意力结构
- 作为缩放点积的基础版本

---

## 1.3 Scaled Dot-Product Attention（Transformer 标配）
**直观理解**：
相当于在1.2的基础上优化。维度越高，Q 和 K 点积出来的分数就越容易爆炸，softmax 就会 “走极端”；除以 √dₖ，就是把分数拉回正常区间，让注意力学得更稳。

**核心做法**：
$$
\text{score} = \frac{q^\top k}{\sqrt{d_k}}
$$

**优点**：
- 数值稳定，解决高维爆炸问题
- 速度快、GPU 友好
- 现代大模型标配

**缺点**：
- 复杂度 \(O(S^2)\)，长序列时显存与计算压力极大

**常用场景**：
- Transformer 编码器/解码器的自注意力、交叉注意力
- 几乎所有 NLP/LLM 基础架构

---

# 2. 按「信息流向」分类：谁看谁

## 2.1 Self-Attention（自注意力）
**直观理解**：
句子里每个词都能和所有词互相建立联系。

**核心做法**：
Q、K、V 都来自同一个序列，每个 token 可以关注全局。

**优点**：
- 擅长建模**长距离依赖**
- 并行度远高于 RNN

**缺点**：
- 依旧是 \(O(S^2)\) 复杂度
- 全局注意力容易分散，需要多层、多头来提炼有效信息

**常用场景**：
- Transformer 编码器（分类、检索、表征学习）
- ViT 视觉Transformer

---

## 2.2 Masked Self-Attention（因果/自回归注意力）
**直观理解**：
写作文时只能看已经写好的内容，不能“偷看未来”。

**核心做法**：
用上三角 mask 屏蔽未来位置，保证只能看到当前及之前的 token。

**优点**：
- 支持逐 token 生成，是语言模型的核心
- 训练时可并行，推理时逐字生成

**缺点**：
- 推理串行，速度受限
- 长上下文 KV Cache 占用显存大

**常用场景**：
- GPT 类解码器架构
- 文本、代码、音乐等自回归生成任务

---

## 2.3 Cross-Attention（交叉注意力）
**直观理解**：
写摘要时，不断回头查阅原文，从中提取依据。

**核心做法**：
- Q 来自解码器
- K、V 来自编码器（或另一模态：图像、音频等）

**优点**：
- 擅长**跨序列对齐**：翻译、摘要、图文对齐
- 让生成“有依据、有条件”

**缺点**：
- 计算量 \(O(S_q S_k)\)，源序列过长时代价高
- 需要缓存 KV，占用显存

**常用场景**：
- 翻译、摘要、问答等 Encoder-Decoder 结构
- 多模态模型（文本查询图像）

---

# 3. 多头注意力 Multi-Head Attention
**直观理解**：
单头是一个专家看问题，多头是一组专家，分别关注语法、指代、长依赖、局部搭配等不同模式。

**核心做法**：
- 将特征维度拆成多个子空间
- 每个头独立计算注意力
- 最后拼接并线性映射回原维度

**优点**：
- 并行学习**多种语义关系**
- 子空间更小，数值更稳定
- 多视角特征融合，表达能力更强

**缺点**：
- 部分头可能失效，注意力趋于平均
- 无法从根本上解决 \(O(S^2)\) 问题

**常用场景**：
- Transformer 所有核心模块：编码器自注意力、解码器掩码自注意力、交叉注意力

---

# 4. 长序列注意力：解决 \(O(S^2)\) 痛点
共同目标：**不让每个 token 都看所有 token**。

## 4.1 Local / Window Attention（局部窗口注意力，Swin 常用）
**直观理解**：只和附近“邻居”交流。

**优点**：复杂度降至 \(O(S\cdot w)\)，显存友好
**缺点**：全局依赖需要多层传递，速度慢
**场景**：Swin Transformer、长文本局部建模

## 4.2 Sparse Attention（稀疏注意力，Longformer/BigBird）
**直观理解**：大部分人只看邻居，少数“全局节点”与所有人交流。

**优点**：兼顾局部与全局，复杂度更低
**缺点**：结构设计偏“人工先验”，实现复杂
**场景**：长文档、法律/论文文本建模

## 4.3 Linear Attention（线性注意力，Performer）
**直观理解**：不做两两匹配，用核函数近似分解，实现线性复杂度。

**优点**：理论 \(O(S)\)，超长序列友好
**缺点**：近似带来精度损失，实现细节敏感
**场景**：超长上下文、流式推理

## 4.4 Low-Rank/Projection Attention（低秩注意力，Linformer）
**直观理解**：先把 K/V 压缩成“摘要”，再做注意力。

**优点**：省算力、省显存
**缺点**：压缩会丢失细节
**场景**：长序列轻量化 Transformer

---