# 1、ReLU 变体

### (1) Leaky ReLU
**表达式：**
$$
\mathrm{LeakyReLU}(x)=
\begin{cases}
x, & x\ge 0 \\
\alpha x, & x<0
\end{cases}
$$

**核心改进：**
解决ReLU的“神经元死亡（Dying ReLU）”问题——对负数输入保留极小斜率 $\alpha$，使神经元在负半区仍能产生梯度，避免完全丧失激活能力。

**优缺点：**
- 优点：计算复杂度低，梯度传递更稳定，可有效缓解神经元失活现象；
- 缺点：斜率 $\alpha$ 为人工预设超参数，不同任务的最优 $\alpha$ 存在差异，无自适应调整能力。

**适用场景：**
作为ReLU的通用“稳健升级版”，在卷积神经网络（CNN）和多层感知机（MLP）中均广泛应用；尤其适用于观测到大量神经元长期输出为0的场景。

---

### (2) PReLU（参数化ReLU）
**表达式：**
$$
\mathrm{PReLU}(x)=
\begin{cases}
x, & x\ge 0 \\
a x, & x<0
\end{cases}
$$
其中 $a$ 为**可学习参数**（可按通道/神经元维度独立设置）。

**核心改进：**
将Leaky ReLU中固定的斜率 $\alpha$ 替换为可学习参数 $a$，由训练过程自动优化（不同通道/神经元可学习到适配自身的负半区斜率）。

**优缺点：**
- 优点：灵活性与自适应性更强，在图像识别等视觉任务中常表现优于Leaky ReLU；
- 缺点：引入额外可学习参数，在小数据集上易增加过拟合风险。

**适用场景：**
在CNN（图像分类、目标检测、语义分割等任务）中应用广泛；适合希望“让负半区斜率由数据本身决定”的场景。

---

### (3) RReLU（随机化Leaky ReLU）
**表达式：**（训练阶段随机取值，推理阶段取期望）
$$
\mathrm{RReLU}(x)=
\begin{cases}
x, & x\ge 0 \\
\alpha x, & x<0
\end{cases}
\quad,\ \alpha \sim \mathcal{U}(l,u)
$$
推理阶段常用取值：
$$
\alpha = \mathbb{E}[\alpha]=\frac{l+u}{2}
$$

**核心改进：**
让负半区斜率在训练过程中随机波动，相当于为激活层引入噪声正则化机制，降低对固定斜率 $\alpha$ 的依赖。

**优缺点：**
- 优点：具备一定正则化效果，可降低过拟合概率，且实现方式简单；
- 缺点：随机性会导致训练过程波动增大，效果增益受任务类型、数据规模影响，稳定性不足。

**适用场景：**
可尝试用于早期小型视觉模型，或希望以“无额外成本”引入轻量正则化的场景；在现代大规模模型中并非主流默认选择。

---

### (4) ReLU6
**表达式：**
$$
\mathrm{ReLU6}(x)=\min(\max(0,x),6)
$$

**核心改进：**
在ReLU基础上增加上界6，将激活输出限制在有限区间内，适配低精度量化需求（尤其是移动端INT8量化部署）。

**优缺点：**
- 优点：对量化操作更友好，限制激活范围可提升数值稳定性；
- 缺点：上界设定会导致正区间出现“饱和”现象（超过6的输入梯度为0），可能削弱模型表达能力。

**适用场景：**
在移动端/嵌入式设备的模型（如轻量化CNN、量化部署场景）中应用广泛。

---

### (5) Thresholded ReLU（阈值ReLU，TReLU）
**表达式：**
$$
\mathrm{TReLU}(x)=
\begin{cases}
x, & x>\theta \\
0, & x\le \theta
\end{cases}
$$

**核心改进：**
将ReLU的激活阈值从0调整为可自定义的 $\theta$，进一步增强激活稀疏性（仅当输入超过阈值时神经元才激活）。

**优缺点：**
- 优点：强化特征稀疏性，可带来一定正则化效果与鲁棒性提升；
- 缺点：阈值 $\theta$ 需人工调优，阈值过大会导致大量神经元长期失活，增加训练难度。

**适用场景：**
适用于对特征稀疏性有明确要求的场景（如部分特征选择、稀疏表示类网络）；一般不作为默认激活函数。

---

### (6) ELU（指数线性单元）
**表达式：**
$$
\mathrm{ELU}(x)=
\begin{cases}
x, & x>0 \\
\alpha(\exp(x)-1), & x\le 0
\end{cases}
$$

**核心改进：**
负区间采用平滑的指数曲线替代硬截断，使输出均值更接近0，缓解内部协变量偏移（Internal Covariate Shift）问题，通常可加速模型收敛。

**优缺点：**
- 优点：有效缓解神经元死亡问题，训练收敛更快、过程更稳定；
- 缺点：包含指数运算，计算成本高于ReLU，GPU并行计算效率略有下降。

**适用场景：**
适用于传统MLP与CNN；适合希望不依赖批归一化（BN），但需更平滑激活曲线、更优输出均值特性的场景。

---

### (7) SELU（缩放指数线性单元）
**表达式：**
$$
\mathrm{SELU}(x)=\lambda
\begin{cases}
x, & x>0 \\
\alpha(\exp(x)-1), & x\le 0
\end{cases}
$$
其中 $\alpha,\lambda$ 为固定常数（经典取值：$\alpha\approx 1.6733,\ \lambda\approx 1.0507$）。

**核心改进：**
具备**自归一化（Self-Normalizing）** 特性——通过缩放系数使各层输出的均值和方差趋于稳定，多数场景下可减少或替代批归一化（BN）。

**优缺点：**
- 优点：训练过程更稳定，可简化网络架构（减少BN层），适配深层MLP；
- 缺点：使用条件较严苛，需配合特定初始化方式（如LeCun Normal）及配套网络设置，否则自归一化优势会失效。

**适用场景：**
适用于深层MLP、以训练稳定性为优先且希望减少BN依赖的场景；在CNN、Transformer中并非默认选择。

---

## 2、除 sigmoid / ReLU / tanh 外的常用激活函数

### (1) GELU（高斯误差线性单元）
**表达式：**
$$
\mathrm{GELU}(x)=x\Phi(x)
$$
常用近似形式：
$$
\mathrm{GELU}(x)\approx \frac{1}{2}x\left(1+\tanh\left(\sqrt{\frac{2}{\pi}}\left(x+0.044715x^3\right)\right)\right)
$$
其中 $\Phi(x)$ 为标准正态分布的累积分布函数（CDF）。

**核心特点：**
- 优点：激活曲线更平滑，梯度传递稳定，适配Transformer架构的特性，表现优异；
- 缺点：计算逻辑更复杂，直观解释性较弱；
- 适用场景：Transformer架构、自然语言处理（NLP）、视觉Transformer（ViT）。

---

### (2) Swish
**表达式：**
$$
\mathrm{Swish}(x)=x\cdot\sigma(x)
$$

**核心特点：**
- 优点：激活曲线平滑且非单调，在部分视觉任务中表现优于传统激活函数；
- 缺点：计算速度慢于ReLU，效果增益受任务影响，稳定性不足；
- 适用场景：CNN、视觉模型、MLP。

---

### (3) Mish
**表达式：**
$$
\mathrm{Mish}(x)=x\cdot\tanh(\ln(1+e^x))
$$
等价形式（引入Softplus函数）：
$$
\mathrm{softplus}(x)=\ln(1+e^x),\quad \mathrm{Mish}(x)=x\cdot\tanh(\mathrm{softplus}(x))
$$

**核心特点：**
- 优点：激活曲线全程平滑、梯度连续，训练过程更稳定；
- 缺点：计算成本较高，工程落地普及度一般；
- 适用场景：部分CNN、目标检测与语义分割模型可选方案。

---

### (4) ELU（指数线性单元）
**表达式：**
$$
\mathrm{ELU}(x)=
\begin{cases}
x, & x>0 \\
\alpha(\exp(x)-1), & x\le 0
\end{cases}
$$
**核心特点：** 同前文“ReLU变体”章节中ELU的优点、缺点及适用场景。

---

### (5) Softplus
**表达式：**
$$
\mathrm{Softplus}(x)=\ln(1+e^x)
$$

**核心特点：**
- 优点：作为ReLU的平滑替代方案，输出恒为正，适配需输出正值参数的场景；
- 缺点：大负数区间梯度趋近于0，输出非零均值，易引发梯度偏移；
- 适用场景：概率建模、不确定性建模（如方差、速率参数估计）。

---

### (6) Softsign
**表达式：**
$$
\mathrm{Softsign}(x)=\frac{x}{1+|x|}
$$

**核心特点：**
- 优点：输出有界，计算成本低；
- 缺点：存在饱和区域，梯度易消失；
- 适用场景：对计算效率要求高、可接受适度梯度饱和的简单模型。

---

### (7) Maxout
**表达式：**
$$
\mathrm{Maxout}(x)=\max_{i=1,\dots,k}(W_i x+b_i)
$$

**核心特点：**
- 优点：模型表达能力强，可拟合任意凸函数；
- 缺点：计算成本高（需并行计算k组线性变换），易过拟合；
- 适用场景：对模型表达力要求高、数据量充足且算力允许的场景。

---

### (8) Linear / Identity（线性激活）
**表达式：**
$$
f(x)=x
$$

**核心特点：**
- 优点：无额外计算开销，输出与输入完全线性映射；
- 缺点：隐藏层使用时，网络会退化为线性模型，无法拟合非线性关系；
- 适用场景：回归任务的输出层（需直接输出连续值）。